{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: SQL Queries and Data Imputation"
      ],
      "metadata": {
        "id": "-6yEyv92Gx0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this starter notebook provides a framework using Python and [DuckDB](https://duckdb.org/docs/api/python/overview.html) to complete this assignment, you may choose to work with any SQL database connector or SQL client of your preference.  \n",
        "\n",
        "If you opt to use a different SQL client, please ensure your submission is:\n",
        "1. A PDF document containing solutions to all questions. Make sure to clearly label each question and its corresponding output\n",
        "2. For each query include:\n",
        "    1. The SQL query you used to obtain the results\n",
        "    2. The first 10 rows of the resulting table\n",
        "    3. The total number of rows and columns in your result set"
      ],
      "metadata": {
        "id": "D75DTHipcvkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction:\n",
        "\n",
        "In this exercise, you will gain hands-on experience with SQL and data imputation techniques.  \n",
        "The dataset `poverty_raw_data.csv` for this exercise includes economic data from various countries (based on modified UN data)."
      ],
      "metadata": {
        "id": "ybGt1Z3WHHCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "NP26J1wnG-pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin by loading the dataset into a pandas DataFrame using the following:"
      ],
      "metadata": {
        "id": "BE8dSo3RIX1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_name = \"poverty_raw_data.csv\"\n",
        "data_df = pd.read_csv(file_name) # Download file and load"
      ],
      "metadata": {
        "id": "gnEuhwUJIP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: SQL Queries on the Raw Data"
      ],
      "metadata": {
        "id": "xwBTKqACI5I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you will write and execute two **SQL queries** on the dataset using a Python library of your choice.  \n",
        "For example, you can use [DuckDB](https://duckdb.org/docs/api/python/overview.html), which allows SQL queries to be run directly on pandas DataFrames."
      ],
      "metadata": {
        "id": "iSs7Tu_NI_qO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Find all the countries in Latin America and the Caribbean (region code LAC) that conducted a national survey (based on the survey_coverage attribute). The query should return the country name, survey acronym, and the first year when the survey was conducted in this country. **Ensure there are no duplicate rows in the result.**"
      ],
      "metadata": {
        "id": "2xL92PcpMdrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here"
      ],
      "metadata": {
        "id": "OOc1g5sSI_Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tWrite an SQL query to find the country (or countries, in case of a tie) in Sub-Saharan Africa (region code SSA) with the lowest average Personal Consumption Expenditures (reporting_pce). The query result should include the country name and its average PCE."
      ],
      "metadata": {
        "id": "gYdogdYPMU1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here"
      ],
      "metadata": {
        "id": "3tet49cNMfez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Imputation"
      ],
      "metadata": {
        "id": "bxaB8lMZKs1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tHandle Missing Data:\n",
        "The dataset includes three attributes with NULL values. For each attribute:\n",
        "*   Choose an imputation method (either using sklearn.impute or your implementation).\n",
        "*   Apply the imputation method to handle the NULL values in the DataFrame.\n",
        "*\t  Justify your choice of imputation methods."
      ],
      "metadata": {
        "id": "wKV3cuYnKz6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here"
      ],
      "metadata": {
        "id": "VO52HwWAM6VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text goes here..."
      ],
      "metadata": {
        "id": "cbXP_zgLNZak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tRe-run the Queries:\n",
        "After imputing the NULL values:\n",
        "*   Re-execute the SQL queries from Part 1 on the updated dataset.\n",
        "* \tDiscuss the results before and after imputation."
      ],
      "metadata": {
        "id": "RgHoWIliM63Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here"
      ],
      "metadata": {
        "id": "H3Efg1IdM7oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text goes here..."
      ],
      "metadata": {
        "id": "RhL3tNr2NaMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Decomposition"
      ],
      "metadata": {
        "id": "yfAGQmJQLjpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will work with the new data after imputation â€“ use the results of an imputation method of your choice from part 2 (as stated above, different methods can be used for different attributes).\n",
        "\n",
        "1.\t**Functional Dependencies:**\n",
        "   \n",
        "   Identify at least two Functional Dependencies (FDs) in the dataset, excluding the record_id column (both FDs may involve the same left-hand side).\n",
        "\n",
        "2.\t**Decompose the Table:**\n",
        "\n",
        "*  Based on the identified FDs, decompose the table into several tables.\n",
        "*  Design and document the database schema for the decomposed table.\n",
        "Submit this as a list of relations (including column names, keys, and foreign keys) or as an ERD.\n",
        "Note: For simplicity, exclude attributes that are not used in the queries, FDs, or imputation.\n",
        "\n",
        "3.\t**Populate the Schema and Re-run Queries:**\n",
        "\n",
        "*  Implement the decomposed schema using the imputed dataset from Part 2.\n",
        "You can use pandas DataFrames (one for each relation) or any other implementation.\n",
        "*  Rewrite the queries from Part 1 to match the decomposed schema.\n",
        "*  Execute the rewritten queries and verify that their results match the results from Part 2.\n",
        "\n",
        "\n",
        "**For the schemas and Diagrams you may add pdf file for submission**"
      ],
      "metadata": {
        "id": "UpKUhoVkLodO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code goes here"
      ],
      "metadata": {
        "id": "BDmAj3pOKzIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text goes here..."
      ],
      "metadata": {
        "id": "8Pra0qMcYvJd"
      }
    }
  ]
}